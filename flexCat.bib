
@article{collins_latent_nodate,
	title = {Latent {Class} and {Latent} {Transition} {Analysis}},
	language = {en},
	author = {Collins, Linda M and Lanza, Stephanie T},
	pages = {332},
	file = {Collins_Lanza_Latent Class and Latent Transition Analysis.pdf:/Users/tasospsy/Google Drive/New_Zotero/Collins_Lanza_Latent Class and Latent Transition Analysis.pdf:application/pdf},
}

@book{finch_latent_2015,
	edition = {0},
	title = {Latent {Variable} {Modeling} with {R}},
	isbn = {978-1-317-97076-7},
	url = {https://www.taylorfrancis.com/books/9781317970767},
	language = {en},
	urldate = {2021-10-09},
	publisher = {Routledge},
	author = {Finch, W. Holmes and French, Brian F.},
	month = jun,
	year = {2015},
	doi = {10.4324/9781315869797},
	file = {Finch_French_2015_Latent Variable Modeling with R.pdf:/Users/tasospsy/Google Drive/New_Zotero/Finch_French_2015_Latent Variable Modeling with R.pdf:application/pdf},
}

@article{kaplan_blocked-cat_2020,
	title = {A {Blocked}-{CAT} {Procedure} for {CD}-{CAT}},
	volume = {44},
	issn = {0146-6216},
	url = {https://doi.org/10.1177/0146621619835500},
	doi = {10.1177/0146621619835500},
	abstract = {This article introduces a blocked-design procedure for cognitive diagnosis computerized adaptive testing (CD-CAT), which allows examinees to review items and change their answers during test administration. Four blocking versions of the new procedure were proposed. In addition, the impact of several factors, namely, item quality, generating model, block size, and test length, on the classification rates was investigated. Three popular item selection indices in CD-CAT were used and their efficiency compared using the new procedure. An additional study was carried out to examine the potential benefit of item review. The results showed that the new procedure is promising in that allowing item review resulted only in a small loss in attribute classification accuracy under some conditions. Moreover, using a blocked-design CD-CAT is beneficial to the extent that it alleviates the negative impact of test anxiety on examinees’ true performance.},
	language = {en},
	number = {1},
	urldate = {2021-11-02},
	journal = {Applied Psychological Measurement},
	author = {Kaplan, Mehmet and de la Torre, Jimmy},
	month = jan,
	year = {2020},
	note = {Publisher: SAGE Publications Inc},
	keywords = {block design, CD-CAT, item review},
	pages = {49--64},
	file = {Kaplan_de la Torre_2020_A Blocked-CAT Procedure for CD-CAT.pdf:/Users/tasospsy/Google Drive/New_Zotero/Kaplan_de la Torre_2020_A Blocked-CAT Procedure for CD-CAT.pdf:application/pdf},
}

@article{van_der_ark_latent_2011,
	title = {A {Latent} {Class} {Approach} to {Estimating} {Test}-{Score} {Reliability}},
	volume = {35},
	issn = {0146-6216},
	url = {https://doi.org/10.1177/0146621610392911},
	doi = {10.1177/0146621610392911},
	abstract = {This study presents a general framework for single-administration reliability methods, such as Cronbach?s alpha, Guttman?s lambda-2, and method MS. This general framework was used to derive a new approach to estimating test-score reliability by means of the unrestricted latent class model. This new approach is the latent class reliability coefficient (LCRC). Unlike other single-administration reliability methods, LCRC places few restrictions on the item scores. A simulation study showed that if data are multidimensional or if double monotonicity does not hold, then LCRC is less biased relative to the true reliability than Cronbach?s alpha, Guttman?s lambda-2, method MS, and the split-half reliability coefficient.},
	number = {5},
	urldate = {2021-11-02},
	journal = {Applied Psychological Measurement},
	author = {van der Ark, L. Andries and van der Palm, Daniël W. and Sijtsma, Klaas},
	month = jul,
	year = {2011},
	note = {Publisher: SAGE Publications Inc},
	pages = {380--392},
	file = {van der Ark et al_2011_A Latent Class Approach to Estimating Test-Score Reliability.pdf:/Users/tasospsy/Google Drive/New_Zotero/van der Ark et al_2011_A Latent Class Approach to Estimating Test-Score Reliability.pdf:application/pdf},
}

@article{sun_new_2021,
	title = {A {New} {Method} to {Balance} {Measurement} {Accuracy} and {Attribute} {Coverage} in {Cognitive} {Diagnostic} {Computerized} {Adaptive} {Testing}},
	issn = {0146-6216},
	url = {https://doi.org/10.1177/01466216211040489},
	doi = {10.1177/01466216211040489},
	abstract = {As one of the important research areas of cognitive diagnosis assessment, cognitive diagnostic computerized adaptive testing (CD-CAT) has received much attention in recent years. Measurement accuracy is the major theme in CD-CAT, and both the item selection method and the attribute coverage have a crucial effect on measurement accuracy. A new attribute coverage index, the ratio of test length to the number of attributes (RTA), is introduced in the current study. RTA is appropriate when the item pool comprises many items that measure multiple attributes where it can both produce acceptable measurement accuracy and balance the attribute coverage. With simulations, the new index is compared to the original item selection method (ORI) and the attribute balance index (ABI), which have been proposed in previous studies. The results show that (1) the RTA method produces comparable measurement accuracy to the ORI method under most item selection methods; (2) the RTA method produces higher measurement accuracy than the ABI method for most item selection methods, with the exception of the mutual information item selection method; (3) the RTA method prefers items that measure multiple attributes, compared to the ORI and ABI methods, while the ABI prefers items that measure a single attribute; and (4) the RTA method performs better than the ORI method with respect to attribute coverage, while it performs worse than the ABI with long tests.},
	urldate = {2021-11-02},
	journal = {Applied Psychological Measurement},
	author = {Sun, Xiaojian and Andersson, Björn and Xin, Tao},
	month = sep,
	year = {2021},
	note = {Publisher: SAGE Publications Inc},
	pages = {01466216211040489},
	file = {Sun et al_2021_A New Method to Balance Measurement Accuracy and Attribute Coverage in.pdf:/Users/tasospsy/Google Drive/New_Zotero/Sun et al_2021_A New Method to Balance Measurement Accuracy and Attribute Coverage in.pdf:application/pdf},
}

@article{chen_note_2020,
	title = {A {Note} on {Likelihood} {Ratio} {Tests} for {Models} with {Latent} {Variables}},
	volume = {85},
	issn = {0033-3123, 1860-0980},
	url = {http://link.springer.com/10.1007/s11336-020-09735-0},
	doi = {10.1007/s11336-020-09735-0},
	abstract = {Abstract
            
              The likelihood ratio test (LRT) is widely used for comparing the relative fit of nested latent variable models. Following Wilks’ theorem, the LRT is conducted by comparing the LRT statistic with its asymptotic distribution under the restricted model, a
              
                
                  \$\${\textbackslash}chi {\textasciicircum}2\$\$
                  
                    
                      χ
                      2
                    
                  
                
              
              distribution with degrees of freedom equal to the difference in the number of free parameters between the two nested models under comparison. For models with latent variables such as factor analysis, structural equation models and random effects models, however, it is often found that the
              
                
                  \$\${\textbackslash}chi {\textasciicircum}2\$\$
                  
                    
                      χ
                      2
                    
                  
                
              
              approximation does not hold. In this note, we show how the regularity conditions of Wilks’ theorem may be violated using three examples of models with latent variables. In addition, a more general theory for LRT is given that provides the correct asymptotic theory for these LRTs. This general theory was first established in Chernoff (J R Stat Soc Ser B (Methodol) 45:404–413, 1954) and discussed in both van der Vaart (Asymptotic statistics, Cambridge, Cambridge University Press, 2000) and Drton (Ann Stat 37:979–1012, 2009), but it does not seem to have received enough attention. We illustrate this general theory with the three examples.},
	language = {en},
	number = {4},
	urldate = {2021-11-02},
	journal = {Psychometrika},
	author = {Chen, Yunxiao and Moustaki, Irini and Zhang, Haoran},
	month = dec,
	year = {2020},
	pages = {996--1012},
	file = {Chen et al_2020_A Note on Likelihood Ratio Tests for Models with Latent Variables.pdf:/Users/tasospsy/Google Drive/New_Zotero/Chen et al_2020_A Note on Likelihood Ratio Tests for Models with Latent Variables.pdf:application/pdf},
}

@article{chen_sparse_2020,
	title = {A {Sparse} {Latent} {Class} {Model} for {Cognitive} {Diagnosis}},
	volume = {85},
	issn = {0033-3123, 1860-0980},
	url = {http://link.springer.com/10.1007/s11336-019-09693-2},
	doi = {10.1007/s11336-019-09693-2},
	abstract = {Cognitive diagnostic models (CDMs) are latent variable models developed to infer latent skills, knowledge, or personalities that underlie responses to educational, psychological, and social science tests and measures. Recent research focused on theory and methods for using sparse latent class models (SLCMs) in an exploratory fashion to infer the latent processes and structure underlying responses. We report new theoretical results about sufﬁcient conditions for generic identiﬁability of SLCM parameters. An important contribution for practice is that our new generic identiﬁability conditions are more likely to be satisﬁed in empirical applications than existing conditions that ensure strict identiﬁability. Learning the underlying latent structure can be formulated as a variable selection problem. We develop a new Bayesian variable selection algorithm that explicitly enforces generic identiﬁability conditions and monotonicity of item response functions to ensure valid posterior inference. We present Monte Carlo simulation results to support accurate inferences and discuss the implications of our ﬁndings for future SLCM research and educational testing.},
	language = {en},
	number = {1},
	urldate = {2021-11-02},
	journal = {Psychometrika},
	author = {Chen, Yinyin and Culpepper, Steven and Liang, Feng},
	month = mar,
	year = {2020},
	pages = {121--153},
	file = {Chen et al_2020_A Sparse Latent Class Model for Cognitive Diagnosis.pdf:/Users/tasospsy/Google Drive/New_Zotero/Chen et al_2020_A Sparse Latent Class Model for Cognitive Diagnosis.pdf:application/pdf},
}

@article{smits_applying_2011,
	title = {Applying computerized adaptive testing to the {CES}-{D} scale: {A} simulation study},
	volume = {188},
	issn = {01651781},
	shorttitle = {Applying computerized adaptive testing to the {CES}-{D} scale},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0165178110007699},
	doi = {10.1016/j.psychres.2010.12.001},
	abstract = {In this paper we studied the appropriateness of developing an adaptive version of the Center of Epidemiological Studies-Depression (CES-D, Radloff, 1977) scale. Computerized Adaptive Testing (CAT) involves the computerized administration of a test in which each item is dynamically selected from a pool of items until a pre-speciﬁed measurement precision is reached. Two types of analyses were performed using the CES-D responses of a large sample of adolescents (N = 1392). First, it was shown that the items met the psychometric requirements needed for CAT. Second, CATs were simulated by using the existing item responses as if they had been collected adaptively. CATs selecting only a small number of items gave results which, in terms of depression measurement and criterion validity, were only marginally different from the results of full CES-D assessment. It was concluded that CAT is a very fruitful way of improving the efﬁciency of the CES-D questionnaire. The discussion addresses the strengths and limitations of the application of CAT in mental health research.},
	language = {en},
	number = {1},
	urldate = {2021-09-21},
	journal = {Psychiatry Research},
	author = {Smits, Niels and Cuijpers, Pim and van Straten, Annemieke},
	month = jun,
	year = {2011},
	pages = {147--155},
	file = {Smits et al_2011_Applying computerized adaptive testing to the CES-D scale.pdf:/Users/tasospsy/Google Drive/New_Zotero/Smits et al_2011_Applying computerized adaptive testing to the CES-D scale.pdf:application/pdf},
}

@article{mcglohen_combining_2008,
	title = {Combining computer adaptive testing technology with cognitively diagnostic assessment},
	volume = {40},
	issn = {1554-351X, 1554-3528},
	url = {http://link.springer.com/10.3758/BRM.40.3.808},
	doi = {10.3758/BRM.40.3.808},
	language = {en},
	number = {3},
	urldate = {2021-11-02},
	journal = {Behavior Research Methods},
	author = {McGlohen, Meghan and Chang, Hua-Hua},
	month = aug,
	year = {2008},
	pages = {808--821},
	file = {McGlohen_Chang_2008_Combining computer adaptive testing technology with cognitively diagnostic.pdf:/Users/tasospsy/Google Drive/New_Zotero/McGlohen_Chang_2008_Combining computer adaptive testing technology with cognitively diagnostic.pdf:application/pdf},
}

@article{nylund_deciding_2007,
	title = {Deciding on the {Number} of {Classes} in {Latent} {Class} {Analysis} and {Growth} {Mixture} {Modeling}: {A} {Monte} {Carlo} {Simulation} {Study}},
	volume = {14},
	issn = {1070-5511},
	shorttitle = {Deciding on the {Number} of {Classes} in {Latent} {Class} {Analysis} and {Growth} {Mixture} {Modeling}},
	url = {https://doi.org/10.1080/10705510701575396},
	doi = {10.1080/10705510701575396},
	abstract = {Mixture modeling is a widely applied data analysis technique used to identify unobserved heterogeneity in a population. Despite mixture models' usefulness in practice, one unresolved issue in the application of mixture models is that there is not one commonly accepted statistical indicator for deciding on the number of classes in a study population. This article presents the results of a simulation study that examines the performance of likelihood-based tests and the traditionally used Information Criterion (ICs) used for determining the number of classes in mixture modeling. We look at the performance of these tests and indexes for 3 types of mixture models: latent class analysis (LCA), a factor mixture model (FMA), and a growth mixture models (GMM). We evaluate the ability of the tests and indexes to correctly identify the number of classes at three different sample sizes (n = 200, 500, 1,000). Whereas the Bayesian Information Criterion performed the best of the ICs, the bootstrap likelihood ratio test proved to be a very consistent indicator of classes across all of the models considered.},
	number = {4},
	urldate = {2021-12-05},
	journal = {Structural Equation Modeling: A Multidisciplinary Journal},
	author = {Nylund, Karen L. and Asparouhov, Tihomir and Muthén, Bengt O.},
	month = oct,
	year = {2007},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/10705510701575396},
	keywords = {READ},
	pages = {535--569},
	file = {Snapshot:/Users/tasospsy/Zotero/storage/WANPSR48/10705510701575396.html:text/html;Nylund et al_2007_Deciding on the Number of Classes in Latent Class Analysis and Growth Mixture.pdf:/Users/tasospsy/Google Drive/New_Zotero/Nylund et al_2007_Deciding on the Number of Classes in Latent Class Analysis and Growth Mixture.pdf:application/pdf},
}

@incollection{fink_determining_2009,
	address = {Berlin, Heidelberg},
	title = {Determining the {Number} of {Components} in {Mixture} {Models} for {Hierarchical} {Data}},
	isbn = {978-3-642-01043-9 978-3-642-01044-6},
	url = {http://link.springer.com/10.1007/978-3-642-01044-6_22},
	abstract = {Recently, various types of mixture models have been developed for data sets having a hierarchical or multilevel structure (see, e,g., [9, 12]). Most of these models include ﬁnite mixture distributions at multiple levels of a hierarchical structure. In these multilevel mixture models, selection of the number of mixture component is more complex than in standard mixture models because one has to determine the number of mixture components at multiple levels.},
	language = {en},
	urldate = {2021-12-07},
	booktitle = {Advances in {Data} {Analysis}, {Data} {Handling} and {Business} {Intelligence}},
	publisher = {Springer Berlin Heidelberg},
	author = {Lukočienė, Olga and Vermunt, Jeroen K.},
	editor = {Fink, Andreas and Lausen, Berthold and Seidel, Wilfried and Ultsch, Alfred},
	year = {2009},
	doi = {10.1007/978-3-642-01044-6_22},
	note = {Series Title: Studies in Classification, Data Analysis, and Knowledge Organization},
	pages = {241--249},
	file = {Lukočienė_Vermunt_2009_Determining the Number of Components in Mixture Models for Hierarchical Data.pdf:/Users/tasospsy/Google Drive/New_Zotero/Lukočienė_Vermunt_2009_Determining the Number of Components in Mixture Models for Hierarchical Data.pdf:application/pdf},
}

@article{kim_determining_2014,
	title = {Determining the {Number} of {Latent} {Classes} in {Single}- and {Multiphase} {Growth} {Mixture} {Models}},
	volume = {21},
	issn = {1070-5511, 1532-8007},
	url = {http://www.tandfonline.com/doi/abs/10.1080/10705511.2014.882690},
	doi = {10.1080/10705511.2014.882690},
	abstract = {Stage-sequential (or multiphase) growth mixture models are useful for delineating potentially different growth processes across multiple phases over time and for determining whether latent subgroups exist within a population. These models are increasingly important as social behavioral scientists are interested in better understanding change processes across distinctively different phases, such as before and after an intervention. One of the less understood issues related to the use of growth mixture models is how to decide on the optimal number of latent classes. The performance of several traditionally used information criteria for determining the number of classes is examined through a Monte Carlo simulation study in single- and multiphase growth mixture models. For thorough examination, the simulation was carried out in 2 perspectives: the models and the factors. The simulation in terms of the models was carried out to see the overall performance of the information criteria within and across the models, whereas the simulation in terms of the factors was carried out to see the effect of each simulation factor on the performance of the information criteria holding the other factors constant. The ﬁndings not only support that sample size adjusted Bayesian Information Criterion would be a good choice under more realistic conditions, such as low class separation, smaller sample size, or missing data, but also increase understanding of the performance of information criteria in single- and multiphase growth mixture models.},
	language = {en},
	number = {2},
	urldate = {2021-12-05},
	journal = {Structural Equation Modeling: A Multidisciplinary Journal},
	author = {Kim, Su-Young},
	month = apr,
	year = {2014},
	pages = {263--279},
	file = {Kim_2014_Determining the Number of Latent Classes in Single- and Multiphase Growth.pdf:/Users/tasospsy/Google Drive/New_Zotero/Kim_2014_Determining the Number of Latent Classes in Single- and Multiphase Growth.pdf:application/pdf},
}

@article{van_der_palm_divisive_2016,
	title = {Divisive {Latent} {Class} {Modeling} as a {Density} {Estimation} {Method} for {Categorical} {Data}},
	volume = {33},
	issn = {0176-4268, 1432-1343},
	url = {http://link.springer.com/10.1007/s00357-016-9195-5},
	doi = {10.1007/s00357-016-9195-5},
	abstract = {Traditionally latent class (LC) analysis is used by applied researchers as a tool for identifying substantively meaningful clusters. More recently, LC models have also been used as a density estimation tool for categorical variables. We introduce a divisive LC (DLC) model as a density estimation tool that may offer several advantages in comparison to a standard LC model. When using an LC model for density estimation, a considerable number of increasingly large LC models may have to be estimated before sufficient model-fit is achieved. A DLC model consists of a sequence of small LC models. Therefore, a DLC model can be estimated much faster and can easily utilize multiple processor cores, meaning that this model is more widely applicable and practical. In this study we describe the algorithm of fitting a DLC model, and discuss the various settings that indirectly influence the precision of a DLC model as a density estimation tool. These settings are illustrated using a synthetic data example, and the best performing algorithm is applied to a real-data example. The generated data example showed that, using specific decision rules, a DLC model is able to correctly model complex associations amongst categorical variables.},
	language = {en},
	number = {1},
	urldate = {2021-09-21},
	journal = {Journal of Classification},
	author = {van der Palm, Daniël W. and van der Ark, L. Andries and Vermunt, Jeroen K.},
	month = apr,
	year = {2016},
	pages = {52--72},
	file = {van der Palm et al_2016_Divisive Latent Class Modeling as a Density Estimation Method for Categorical.pdf:/Users/tasospsy/Google Drive/New_Zotero/van der Palm et al_2016_Divisive Latent Class Modeling as a Density Estimation Method for Categorical.pdf:application/pdf},
}

@article{he_evaluating_2019,
	title = {Evaluating the {Performance} of the {K}-fold {Cross}-{Validation} {Approach} for {Model} {Selection} in {Growth} {Mixture} {Modeling}},
	volume = {26},
	issn = {1070-5511},
	url = {https://doi.org/10.1080/10705511.2018.1500140},
	doi = {10.1080/10705511.2018.1500140},
	abstract = {Deciding on the number of “classes” has been the most prominent and most debated challenge in finite mixture modeling. Recently, a novel strategy has been proposed to select the best model in finite mixture modeling: a k-fold cross-validation approach. However, this approach has not been systematically evaluated, which makes the performance of the k-fold cross-validation approach for model selection in finite mixture modeling largely unknown. Thus, the main motivation for conducting the current work is to systematically evaluate the performance of the k-fold cross-validation approach for model selection in the context of Growth Mixture Modeling. Results revealed that the performance of the k-fold cross-validation approach for model selection in GMM is generally unsatisfactory, and it only performs reasonably well under the condition of very large class separation.},
	number = {1},
	urldate = {2021-12-05},
	journal = {Structural Equation Modeling: A Multidisciplinary Journal},
	author = {He, Jinbo and Fan, Xitao},
	month = jan,
	year = {2019},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/10705511.2018.1500140},
	keywords = {Growth mixture modeling, k-fold cross-validation approach, model selection},
	pages = {66--79},
	file = {Snapshot:/Users/tasospsy/Zotero/storage/8T3PN29W/10705511.2018.html:text/html;He_Fan_2019_Evaluating the Performance of the K-fold Cross-Validation Approach for Model.pdf:/Users/tasospsy/Google Drive/New_Zotero/He_Fan_2019_Evaluating the Performance of the K-fold Cross-Validation Approach for Model.pdf:application/pdf},
}

@article{whittaker_exploring_2021,
	title = {Exploring the {Enumeration} {Accuracy} of {Cross}-{Validation} {Indices} in {Latent} {Class} {Analysis}},
	volume = {28},
	issn = {1070-5511},
	url = {https://doi.org/10.1080/10705511.2020.1802280},
	doi = {10.1080/10705511.2020.1802280},
	abstract = {A crucial issue when estimating mixture models is selecting the model with the correct number of latent classes underlying the data, which is commonly referred to as class enumeration. Although cross-validation methods have been suggested with mixture models to help augment the class enumeration process (Masyn, 2013), they have been seldom used. The purpose of this simulation study was to compare the performance of traditionally used single sample enumeration indices with the performance of cross-validation indices when selecting the correct latent class model with binary indicators. Various conditions were manipulated, including the number of indicators, sample size, class separation, mixing proportions, and number of latent classes. The enumeration accuracy of sixteen indices (traditional, cross-validated, and double cross-validated) were documented in the manipulated conditions. The traditional sample-size adjusted BIC index was the most accurate among the indices. The performance of the double cross-validated -2LL was also promising. Recommendations are provided.},
	number = {3},
	urldate = {2021-12-05},
	journal = {Structural Equation Modeling: A Multidisciplinary Journal},
	author = {Whittaker, Tiffany A. and Miller, J. E.},
	month = may,
	year = {2021},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/10705511.2020.1802280},
	keywords = {latent class analysis, Class enumeration, cross-validation},
	pages = {376--390},
	file = {Snapshot:/Users/tasospsy/Zotero/storage/Y7EDR2GA/10705511.2020.html:text/html;Whittaker_Miller_2021_Exploring the Enumeration Accuracy of Cross-Validation Indices in Latent Class.pdf:/Users/tasospsy/Google Drive/New_Zotero/Whittaker_Miller_2021_Exploring the Enumeration Accuracy of Cross-Validation Indices in Latent Class.pdf:application/pdf},
}

@article{porcu_introduction_2017,
	title = {Introduction to {Latent} {Class} {Analysis} {With} {Applications}},
	volume = {37},
	issn = {0272-4316},
	url = {https://doi.org/10.1177/0272431616648452},
	doi = {10.1177/0272431616648452},
	abstract = {Latent class analysis (LCA) is a statistical method used to group individuals (cases, units) into classes (categories) of an unobserved (latent) variable on the basis of the responses made on a set of nominal, ordinal, or continuous observed variables. In this article, we introduce LCA in order to demonstrate its usefulness to early adolescence researchers. We provide an application of LCA to empirical data collected from a national survey carried out in 2010 in Italy to assess mathematics and reading skills of fifth-grade primary school pupils (10 years in age). The data were used to measure pupils’ supplies of cultural capital by specifying a latent class model. This article aims to describe and interpret results of LCA, allowing users to replicate the analysis. All LCA examples included in the text are illustrated using the Latent GOLD package, and command files needed to reproduce all analyses with SAS and R are available as supplemental online appendix files along with the example data files.},
	language = {en},
	number = {1},
	urldate = {2021-10-09},
	journal = {The Journal of Early Adolescence},
	author = {Porcu, Mariano and Giambona, Francesca},
	month = jan,
	year = {2017},
	note = {Publisher: SAGE Publications Inc},
	keywords = {cross-sectional, latent class analysis, latent GOLD, pedagogical, R, SAS},
	pages = {129--158},
	file = {Porcu_Giambona_2017_Introduction to Latent Class Analysis With Applications.pdf:/Users/tasospsy/Google Drive/New_Zotero/Porcu_Giambona_2017_Introduction to Latent Class Analysis With Applications.pdf:application/pdf},
}

@incollection{von_davier_introduction_2019,
	address = {Cham},
	series = {Methodology of {Educational} {Measurement} and {Assessment}},
	title = {Introduction: {From} {Latent} {Classes} to {Cognitive} {Diagnostic} {Models}},
	isbn = {978-3-030-05584-4},
	shorttitle = {Introduction},
	url = {https://doi.org/10.1007/978-3-030-05584-4_1},
	abstract = {This chapter provides historical and structural context for models and approaches presented in this volume, by presenting an overview of important predecessors of diagnostic classification models which we will refer to as DCM in this volume, or alternatively cognitive diagnostic models (CDMs). The chapter covers general notation and concepts central to latent class analysis, followed by an introduction of mastery models, ranging from deterministic to probabilistic forms. The ensuing sections cover knowledge state and rule space approaches, which can be viewed as deterministic skill-profile models. The chapter closes with a section on the multiple classification latent class model and the deterministic input noisy and (DINA) model.},
	language = {en},
	urldate = {2021-11-01},
	booktitle = {Handbook of {Diagnostic} {Classification} {Models}: {Models} and {Model} {Extensions}, {Applications}, {Software} {Packages}},
	publisher = {Springer International Publishing},
	author = {von Davier, Matthias and Lee, Young-Sun},
	editor = {von Davier, Matthias and Lee, Young-Sun},
	year = {2019},
	doi = {10.1007/978-3-030-05584-4_1},
	pages = {1--17},
}

@article{borsboom_kinds_2016,
	title = {Kinds \textit{versus} continua: a review of psychometric approaches to uncover the structure of psychiatric constructs},
	volume = {46},
	issn = {0033-2917, 1469-8978},
	shorttitle = {Kinds \textit{versus} continua},
	url = {https://www.cambridge.org/core/product/identifier/S0033291715001944/type/journal_article},
	doi = {10.1017/S0033291715001944},
	abstract = {The question of whether psychopathology constructs are discrete kinds or continuous dimensions represents an important issue in clinical psychology and psychiatry. The present paper reviews psychometric modelling approaches that can be used to investigate this question through the application of statistical models. The relation between constructs and indicator variables in models with categorical and continuous latent variables is discussed, as are techniques specifically designed to address the distinction between latent categories as opposed to continua (taxometrics). In addition, we examine latent variable models that allow latent structures to have both continuous and categorical characteristics, such as factor mixture models and grade-of-membership models. Finally, we discuss recent alternative approaches based on network analysis and dynamical systems theory, which entail that the structure of constructs may be continuous for some individuals but categorical for others. Our evaluation of the psychometric literature shows that the kinds–continua distinction is considerably more subtle than is often presupposed in research; in particular, the hypotheses of kinds and continua are not mutually exclusive or exhaustive. We discuss opportunities to go beyond current research on the issue by using dynamical systems models, intra-individual time series and experimental manipulations.},
	language = {en},
	number = {8},
	urldate = {2021-10-09},
	journal = {Psychological Medicine},
	author = {Borsboom, D. and Rhemtulla, M. and Cramer, A. O. J. and van der Maas, H. L. J. and Scheffer, M. and Dolan, C. V.},
	month = jun,
	year = {2016},
	pages = {1567--1579},
	file = {Borsboom et al_2016_Kinds iversus-i continua.pdf:/Users/tasospsy/Google Drive/New_Zotero/Borsboom et al_2016_Kinds iversus-i continua.pdf:application/pdf},
}

@incollection{vermunt_latent_2004,
	address = {Thousand Oakes},
	title = {Latent class analysis},
	isbn = {978-0-7619-2363-3},
	booktitle = {The {SAGE} {Handbook} of {Quantitative} {Methodology} for the {Social} {Sciences}},
	publisher = {Sage},
	author = {Vermunt, J.K. and Magidson, J.},
	editor = {Lewis-Beck, M. and Bryman, A. and Liao, T.F.},
	year = {2004},
	keywords = {READ},
	pages = {549--553},
	file = {Vermunt_Magidson_2004_Latent class analysis.pdf:/Users/tasospsy/Google Drive/New_Zotero/Vermunt_Magidson_2004_Latent class analysis.pdf:application/pdf},
}

@article{xu_latent_2018,
	title = {Latent {Class} {Analysis} of {Recurrent} {Events} in {Problem}-{Solving} {Items}},
	volume = {42},
	issn = {0146-6216},
	url = {https://doi.org/10.1177/0146621617748325},
	doi = {10.1177/0146621617748325},
	abstract = {Computer-based assessment of complex problem-solving abilities is becoming more and more popular. In such an assessment, the entire problem-solving process of an examinee is recorded, providing detailed information about the individual, such as behavioral patterns, speed, and learning trajectory. The problem-solving processes are recorded in a computer log file which is a time-stamped documentation of events related to task completion. As opposed to cross-sectional response data from traditional tests, process data in log files are massive and irregularly structured, calling for effective exploratory data analysis methods. Motivated by a specific complex problem-solving item ?Climate Control? in the 2012 Programme for International Student Assessment, the authors propose a latent class analysis approach to analyzing the events occurred in the problem-solving processes. The exploratory latent class analysis yields meaningful latent classes. Simulation studies are conducted to evaluate the proposed approach.},
	number = {6},
	urldate = {2021-11-02},
	journal = {Applied Psychological Measurement},
	author = {Xu, Haochen and Fang, Guanhua and Chen, Yunxiao and Liu, Jingchen and Ying, Zhiliang},
	month = sep,
	year = {2018},
	note = {Publisher: SAGE Publications Inc},
	pages = {478--498},
	file = {Xu et al_2018_Latent Class Analysis of Recurrent Events in Problem-Solving Items.pdf:/Users/tasospsy/Google Drive/New_Zotero/Xu et al_2018_Latent Class Analysis of Recurrent Events in Problem-Solving Items.pdf:application/pdf},
}

@article{smits_measurement_2018,
	title = {Measurement versus prediction in the construction of patient-reported outcome questionnaires: can we have our cake and eat it?},
	volume = {27},
	issn = {0962-9343, 1573-2649},
	shorttitle = {Measurement versus prediction in the construction of patient-reported outcome questionnaires},
	url = {http://link.springer.com/10.1007/s11136-017-1720-4},
	doi = {10.1007/s11136-017-1720-4},
	language = {en},
	number = {7},
	urldate = {2021-09-21},
	journal = {Quality of Life Research},
	author = {Smits, Niels and van der Ark, L. Andries and Conijn, Judith M.},
	month = jul,
	year = {2018},
	pages = {1673--1682},
	file = {Smits et al_2018_Measurement versus prediction in the construction of patient-reported outcome.pdf:/Users/tasospsy/Google Drive/New_Zotero/Smits et al_2018_Measurement versus prediction in the construction of patient-reported outcome.pdf:application/pdf},
}

@incollection{robertson_mixture_2016,
	address = {Cham},
	title = {Mixture {Models}: {Latent} {Profile} and {Latent} {Class} {Analysis}},
	isbn = {978-3-319-26631-2 978-3-319-26633-6},
	shorttitle = {Mixture {Models}},
	url = {http://link.springer.com/10.1007/978-3-319-26633-6_12},
	abstract = {Latent class analysis (LCA) and latent proﬁle analysis (LPA) are techniques that aim to recover hidden groups from observed data. They are similar to clustering techniques but more ﬂexible because they are based on an explicit model of the data, and allow you to account for the fact that the recovered groups are uncertain. LCA and LPA are useful when you want to reduce a large number of continuous (LPA) or categorical (LCA) variables to a few subgroups. They can also help experimenters in situations where the treatment effect is different for different people, but we do not know which people. This chapter explains how LPA and LCA work, what assumptions are behind the techniques, and how you can use R to apply them.},
	language = {en},
	urldate = {2021-09-21},
	booktitle = {Modern {Statistical} {Methods} for {HCI}},
	publisher = {Springer International Publishing},
	author = {Oberski, Daniel},
	editor = {Robertson, Judy and Kaptein, Maurits},
	year = {2016},
	doi = {10.1007/978-3-319-26633-6_12},
	note = {Series Title: Human–Computer Interaction Series},
	keywords = {READ},
	pages = {275--287},
	file = {Oberski_2016_Mixture Models.pdf:/Users/tasospsy/Google Drive/New_Zotero/Oberski_2016_Mixture Models.pdf:application/pdf},
}

@article{grimm_model_2021,
	title = {Model {Fit} and {Comparison} in {Finite} {Mixture} {Models}: {A} {Review} and a {Novel} {Approach}},
	volume = {6},
	issn = {2504-284X},
	shorttitle = {Model {Fit} and {Comparison} in {Finite} {Mixture} {Models}},
	url = {https://www.frontiersin.org/article/10.3389/feduc.2021.613645},
	doi = {10.3389/feduc.2021.613645},
	abstract = {One of the greatest challenges in the application of finite mixture models is model comparison. A variety of statistical fit indices exist, including information criteria, approximate likelihood ratio tests, and resampling techniques; however, none of these indices describe the amount of improvement in model fit when a latent class is added to the model. We review these model fit statistics and propose a novel approach, the likelihood increment percentage per parameter (LIPpp), targeting the relative improvement in model fit when a class is added to the model. Simulation work based on two previous simulation studies highlighted the potential for the LIPpp to identify the correct number of classes, and provide context for the magnitude of improvement in model fit. We conclude with recommendations and future research directions.},
	urldate = {2021-12-05},
	journal = {Frontiers in Education},
	author = {Grimm, Kevin J. and Houpt, Russell and Rodgers, Danielle},
	year = {2021},
	pages = {27},
	file = {Grimm et al_2021_Model Fit and Comparison in Finite Mixture Models.pdf:/Users/tasospsy/Google Drive/New_Zotero/Grimm et al_2021_Model Fit and Comparison in Finite Mixture Models.pdf:application/pdf},
}

@article{grimm_model_2017,
	title = {Model {Selection} in {Finite} {Mixture} {Models}: {A} \textit{k} -{Fold} {Cross}-{Validation} {Approach}},
	volume = {24},
	issn = {1070-5511, 1532-8007},
	shorttitle = {Model {Selection} in {Finite} {Mixture} {Models}},
	url = {https://www.tandfonline.com/doi/full/10.1080/10705511.2016.1250638},
	doi = {10.1080/10705511.2016.1250638},
	abstract = {Finite mixture models, whether latent class models, growth mixture models, latent proﬁle models, or factor mixture models, have become an important statistical tool in social science research. One of the biggest and most debated challenges in mixture modeling is the evaluation of model ﬁt and model comparison. In the application of mixture models, researchers often ﬁt a collection of models and then decide on a single optimal model based on a variety of model ﬁt information. We propose a k-fold cross-validation procedure to model selection whereby the model is repeatedly ﬁt to k À 1 different partitions of the data set, the resulting model is then applied to kth partition of the sample, and the distribution of ﬁt indexes is examined. This method is illustrated with growth mixture models ﬁt to longitudinal data on reading ability collected as part of the Early Childhood Longitudinal Study–Kindergarten Cohort.},
	language = {en},
	number = {2},
	urldate = {2021-12-05},
	journal = {Structural Equation Modeling: A Multidisciplinary Journal},
	author = {Grimm, Kevin J. and Mazza, Gina L. and Davoudzadeh, Pega},
	month = mar,
	year = {2017},
	pages = {246--256},
	file = {Grimm et al_2017_Model Selection in Finite Mixture Models.pdf:/Users/tasospsy/Google Drive/New_Zotero/Grimm et al_2017_Model Selection in Finite Mixture Models.pdf:application/pdf},
}

@article{chen_mutual_2018,
	title = {Mutual {Information} {Reliability} for {Latent} {Class} {Analysis}},
	volume = {42},
	issn = {0146-6216},
	url = {https://doi.org/10.1177/0146621617748324},
	doi = {10.1177/0146621617748324},
	abstract = {Latent class models are powerful tools in psychological and educational measurement. These models classify individuals into subgroups based on a set of manifest variables, assisting decision making in a diagnostic system. In this article, based on information theory, the authors propose a mutual information reliability (MIR) coefficient that summaries the measurement quality of latent class models, where the latent variables being measured are categorical. The proposed coefficient is analogous to a version of reliability coefficient for item response theory models and meets the general concept of measurement reliability in the Standards for Educational and Psychological Testing. The proposed coefficient can also be viewed as an extension of the McFadden?s pseudo R-square coefficient, which evaluates the goodness-of-fit of logistic regression model, to latent class models. Thanks to several information-theoretic inequalities, the MIR coefficient is unitless, lies between 0 and 1, and receives good interpretation from a measurement point of view. The coefficient can be applied to both fixed and computerized adaptive testing designs. The performance of the MIR coefficient is demonstrated by simulated examples.},
	number = {6},
	urldate = {2021-11-02},
	journal = {Applied Psychological Measurement},
	author = {Chen, Yunxiao and Liu, Yang and Xu, Shuangshuang},
	month = sep,
	year = {2018},
	note = {Publisher: SAGE Publications Inc},
	pages = {460--477},
	file = {Chen et al_2018_Mutual Information Reliability for Latent Class Analysis.pdf:/Users/tasospsy/Google Drive/New_Zotero/Chen et al_2018_Mutual Information Reliability for Latent Class Analysis.pdf:application/pdf},
}

@article{kaplan_new_2015,
	title = {New {Item} {Selection} {Methods} for {Cognitive} {Diagnosis} {Computerized} {Adaptive} {Testing}},
	doi = {10.1177/0146621614554650},
	abstract = {Two new item selection methods, the modified posterior-weighted Kullback–Leibler index (MPWKL) and the generalized deterministic inputs, noisy “and” gate (G-DINA) model discrimination index (GDI), that can be used in cognitive diagnosis computerized adaptive testing are introduced. This article introduces two new item selection methods, the modified posterior-weighted Kullback–Leibler index (MPWKL) and the generalized deterministic inputs, noisy “and” gate (G-DINA) model discrimination index (GDI), that can be used in cognitive diagnosis computerized adaptive testing. The efficiency of the new methods is compared with the posterior-weighted Kullback–Leibler (PWKL) item selection index using a simulation study in the context of the G-DINA model. The impact of item quality, generating models, and test termination rules on attribute classification accuracy or test length is also investigated. The results of the study show that the MPWKL and GDI perform very similarly, and have higher correct attribute classification rates or shorter mean test lengths compared with the PWKL. In addition, the GDI has the shortest implementation time among the three indices. The proportion of item usage with respect to the required attributes across the different conditions is also tracked and discussed.},
	journal = {Applied psychological measurement},
	author = {Kaplan, Mehmet and Torre, Jimmy de la and Barrada, J.},
	year = {2015},
	file = {Kaplan et al_2015_New Item Selection Methods for Cognitive Diagnosis Computerized Adaptive Testing.pdf:/Users/tasospsy/Google Drive/New_Zotero/Kaplan et al_2015_New Item Selection Methods for Cognitive Diagnosis Computerized Adaptive Testing.pdf:application/pdf},
}

@article{chang_nonparametric_2019,
	title = {Nonparametric {CAT} for {CD} in {Educational} {Settings} {With} {Small} {Samples}},
	volume = {43},
	issn = {0146-6216},
	url = {https://doi.org/10.1177/0146621618813113},
	doi = {10.1177/0146621618813113},
	abstract = {Cognitive diagnostic computerized adaptive testing (CD-CAT) has been suggested by researchers as a diagnostic tool for assessment and evaluation. Although model-based CD-CAT is relatively well researched in the context of large-scale assessment systems, this type of system has not received the same degree of research and development in small-scale settings, such as at the course-based level, where this system would be the most useful. The main obstacle is that the statistical estimation techniques that are successfully applied within the context of a large-scale assessment require large samples to guarantee reliable calibration of the item parameters and an accurate estimation of the examinees’ proficiency class membership. Such samples are simply not obtainable in course-based settings. Therefore, the nonparametric item selection (NPS) method that does not require any parameter calibration, and thus, can be used in small educational programs is proposed in the study. The proposed nonparametric CD-CAT uses the nonparametric classification (NPC) method to estimate an examinee’s attribute profile and based on the examinee’s item responses, the item that can best discriminate the estimated attribute profile and the other attribute profiles is then selected. The simulation results show that the NPS method outperformed the compared parametric CD-CAT algorithms and the differences were substantial when the calibration samples were small.},
	language = {en},
	number = {7},
	urldate = {2021-11-02},
	journal = {Applied Psychological Measurement},
	author = {Chang, Yuan-Pei and Chiu, Chia-Yi and Tsai, Rung-Ching},
	month = oct,
	year = {2019},
	note = {Publisher: SAGE Publications Inc},
	keywords = {cognitive diagnosis, computerized adaptive testing, nonparametric classification, nonparametric item selection},
	pages = {543--561},
	file = {Chang et al_2019_Nonparametric CAT for CD in Educational Settings With Small Samples.pdf:/Users/tasospsy/Google Drive/New_Zotero/Chang et al_2019_Nonparametric CAT for CD in Educational Settings With Small Samples.pdf:application/pdf},
}

@incollection{van_der_ark_nonparametric_2019,
	address = {Cham},
	series = {Methodology of {Educational} {Measurement} and {Assessment}},
	title = {Nonparametric {Item} {Response} {Theory} and {Mokken} {Scale} {Analysis}, with {Relations} to {Latent} {Class} {Models} and {Cognitive} {Diagnostic} {Models}},
	isbn = {978-3-030-05584-4},
	url = {https://doi.org/10.1007/978-3-030-05584-4_2},
	abstract = {As the focus of this chapter, we discuss nonparametric item response theory for ordinal person scales, specifically the monotone homogeneity model and Mokken scale analysis, which is the data-analysis procedure used for investigating the compliance between the monotone homogeneity model and data. Next, we discuss the unrestricted latent class model as an even more liberal model for investigating the scalability of a set of items, producing nominal scales, but we also discuss an ordered latent class model that one can use to investigate assumptions about item response functions in the monotone homogeneity model and other nonparametric item response models. Finally, we discuss cognitive diagnostic models, which are the core of this volume, and which are a further deepening of latent class models, providing diagnostic information about the people who responded to a set of items. A data analysis example, using item scores of 1210 respondents on 44 items from the Millon Clinical Multiaxial Inventory III, demonstrates how the monotone homogeneity model, the latent class model, and two cognitive diagnostic models can be used jointly to understand one’s data.},
	language = {en},
	urldate = {2021-11-01},
	booktitle = {Handbook of {Diagnostic} {Classification} {Models}: {Models} and {Model} {Extensions}, {Applications}, {Software} {Packages}},
	publisher = {Springer International Publishing},
	author = {van der Ark, L. Andries and Rossi, Gina and Sijtsma, Klaas},
	editor = {von Davier, Matthias and Lee, Young-Sun},
	year = {2019},
	doi = {10.1007/978-3-030-05584-4_2},
	pages = {21--45},
	file = {van der Ark et al_2019_Nonparametric Item Response Theory and Mokken Scale Analysis, with Relations to.pdf:/Users/tasospsy/Google Drive/New_Zotero/van der Ark et al_2019_Nonparametric Item Response Theory and Mokken Scale Analysis, with Relations to.pdf:application/pdf},
}

@article{wang_interim_2021,
	title = {On {Interim} {Cognitive} {Diagnostic} {Computerized} {Adaptive} {Testing} in {Learning} {Context}},
	volume = {45},
	issn = {0146-6216},
	url = {https://doi.org/10.1177/0146621621990755},
	doi = {10.1177/0146621621990755},
	abstract = {Interim assessment occurs throughout instruction to provide feedback about what students know and have achieved. Different from the current available cognitive diagnostic computerized adaptive testing (CD-CAT) design that focuses on assessment at a single time point, the authors discuss several designs of interim CD-CAT that are suitable in the learning context. The interim CD-CAT differs from the current available CD-CAT designs primarily because students’ mastery profile (i.e., skills mastery) changes due to learning, and new attributes are added periodically. Moreover, hierarchies exist among attributes taught sequentially and such information could be used during item selection. Two specific designs are considered: The first one is when new attributes are taught in Stage II, but the student mastery status of the previously taught attributes stays the same. The second design is when both new attributes are taught, and previously taught attributes can be further learned or forgotten in Stage II. For both designs, the authors propose an individual prior, which considers a person’s learning history and population learning model, to start an interim CD-CAT. Simulation results show that the Stage II CD-CAT using individual prior outperforms the methods using population priors. The GDINA (generalized deterministic inputs, noisy, “and” gate) diagnostic index (GDI) is extended to accommodate item hierarchies, and analytic results are provided to further illustrate the types of items that are most popular during item selection. As the first study that focuses on the application of CD-CAT in a learning context, the methods and results present herein showed the great promise of using CD-CAT to monitor learning.},
	language = {en},
	number = {4},
	urldate = {2021-11-02},
	journal = {Applied Psychological Measurement},
	author = {Wang, Chun},
	month = jun,
	year = {2021},
	note = {Publisher: SAGE Publications Inc},
	keywords = {learning, cognitive diagnostic computerized adaptive testing, item selection},
	pages = {235--252},
	file = {Wang_2021_On Interim Cognitive Diagnostic Computerized Adaptive Testing in Learning.pdf:/Users/tasospsy/Google Drive/New_Zotero/Wang_2021_On Interim Cognitive Diagnostic Computerized Adaptive Testing in Learning.pdf:application/pdf},
}

@article{falk_performance_2021,
	title = {On the {Performance} of {Semi}- and {Nonparametric} {Item} {Response} {Functions} in {Computer} {Adaptive} {Tests}},
	issn = {0013-1644},
	url = {https://doi.org/10.1177/00131644211014261},
	doi = {10.1177/00131644211014261},
	abstract = {Large-scale assessments often use a computer adaptive test (CAT) for selection of items and for scoring respondents. Such tests often assume a parametric form for the relationship between item responses and the underlying construct. Although semi- and nonparametric response functions could be used, there is scant research on their performance in a CAT. In this work, we compare parametric response functions versus those estimated using kernel smoothing and a logistic function of a monotonic polynomial. Monotonic polynomial items can be used with traditional CAT item selection algorithms that use analytical derivatives. We compared these approaches in CAT simulations with a variety of item selection algorithms. Our simulations also varied the features of the calibration and item pool: sample size, the presence of missing data, and the percentage of nonstandard items. In general, the results support the use of semi- and nonparametric item response functions in a CAT.},
	language = {en},
	urldate = {2021-11-21},
	journal = {Educational and Psychological Measurement},
	author = {Falk, Carl F. and Feuerstahler, Leah M.},
	month = may,
	year = {2021},
	note = {Publisher: SAGE Publications Inc},
	keywords = {computer adaptive test, large-scale testing, monotonic polynomial, nonparametric IRT},
	pages = {00131644211014261},
	file = {Falk_Feuerstahler_2021_On the Performance of Semi- and Nonparametric Item Response Functions in.pdf:/Users/tasospsy/Google Drive/New_Zotero/Falk_Feuerstahler_2021_On the Performance of Semi- and Nonparametric Item Response Functions in.pdf:application/pdf},
}

@misc{noauthor_psychometric_nodate,
	title = {Psychometric and machine learning approaches for diagnostic assessment and tests of individual classification. - {PsycNET}},
	url = {https://psycnet.apa.org/doiLanding?doi=10.1037%2Fmet0000317},
	urldate = {2021-12-05},
	file = {Psychometric and machine learning approaches for diagnostic assessment and tests of individual classification. - PsycNET:/Users/tasospsy/Zotero/storage/BUEGP7Z3/doiLanding.html:text/html},
}

@article{gonulates_quality_2019,
	title = {Quality of {Item} {Pool} ({QIP}) {Index}: {A} {Novel} {Approach} to {Evaluating} {CAT} {Item} {Pool} {Adequacy}},
	volume = {79},
	issn = {0013-1644},
	shorttitle = {Quality of {Item} {Pool} ({QIP}) {Index}},
	url = {https://doi.org/10.1177/0013164419842215},
	doi = {10.1177/0013164419842215},
	abstract = {This article introduces the Quality of Item Pool (QIP) Index, a novel approach to quantifying the adequacy of an item pool of a computerized adaptive test for a given set of test specifications and examinee population. This index ranges from 0 to 1, with values close to 1 indicating the item pool presents optimum items to examinees throughout the test. This index can be used to compare different item pools or diagnose the deficiencies of a given item pool by quantifying the amount of deviation from a perfect item pool. Simulation studies were conducted to evaluate the capacity of this index for detecting the inadequacies of two simulated item pools. The value of this index was compared with the existing methods of evaluating the quality of computerized adaptive tests (CAT). Results of the study showed that the QIP Index can detect even slight deviations between a proposed item pool and an optimal item pool. It can also uncover shortcomings of an item pool that other outcomes of CAT cannot detect. CAT developers can use the QIP Index to diagnose the weaknesses of the item pool and as a guide for improving item pools.},
	language = {en},
	number = {6},
	urldate = {2021-11-21},
	journal = {Educational and Psychological Measurement},
	author = {Gönülateş, Emre},
	month = dec,
	year = {2019},
	note = {Publisher: SAGE Publications Inc},
	keywords = {computerized adaptive testing, item pool design, item pool evaluation},
	pages = {1133--1155},
	file = {Gönülateş_2019_Quality of Item Pool (QIP) Index.pdf:/Users/tasospsy/Google Drive/New_Zotero/Gönülateş_2019_Quality of Item Pool (QIP) Index.pdf:application/pdf},
}

@article{chen_regularized_2016,
	title = {{REGULARIZED} {LATENT} {CLASS} {ANALYSIS} {WITH} {APPLICATION} {IN} {COGNITIVE} {DIAGNOSIS}},
	issn = {0033-3123},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5513805/},
	doi = {10.1007/s11336-016-9545-6},
	abstract = {Diagnostic classification models are confirmatory in the sense that the relationship between the latent attributes and responses to items is specified or parameterized. Such models are readily interpretable with each component of the model usually having a practical meaning. However; parameterized diagnostic classification models are sometimes too simple to capture all the data patterns, resulting in significant model lack of fit. In this paper, we attempt to obtain a compromise between interpretability and goodness of fit by regularizing a latent class model. Our approach starts with minimal assumptions on the data structure, followed by suitable regularization to reduce complexity, so that readily interpretable, yet flexible model is obtained. An expectation–maximization-type algorithm is developed for efficient computation. It is shown that the proposed approach enjoys good theoretical properties. Results from simulation studies and a real application are presented.},
	urldate = {2021-11-02},
	journal = {Psychometrika},
	author = {Chen, Yunxiao and Li, Xiaoou and Liu, Jingchen and Ying, Zhiliang},
	month = nov,
	year = {2016},
	pmid = {27905058},
	pmcid = {PMC5513805},
	pages = {10.1007/s11336--016--9545--6},
	file = {Chen et al_2016_REGULARIZED LATENT CLASS ANALYSIS WITH APPLICATION IN COGNITIVE DIAGNOSIS.pdf:/Users/tasospsy/Google Drive/New_Zotero/Chen et al_2016_REGULARIZED LATENT CLASS ANALYSIS WITH APPLICATION IN COGNITIVE DIAGNOSIS.pdf:application/pdf},
}

@article{petersen_application_2019,
	title = {The {Application} of {Latent} {Class} {Analysis} for {Investigating} {Population} {Child} {Mental} {Health}: {A} {Systematic} {Review}},
	volume = {10},
	issn = {1664-1078},
	shorttitle = {The {Application} of {Latent} {Class} {Analysis} for {Investigating} {Population} {Child} {Mental} {Health}},
	url = {https://www.frontiersin.org/article/10.3389/fpsyg.2019.01214/full},
	doi = {10.3389/fpsyg.2019.01214},
	abstract = {Background: Latent class analysis (LCA) can be used to identify subgroups of children with similar patterns of mental health symptoms and/or strengths. The method is becoming more commonly used in child mental health research, but there are reservations about the replicability, reliability, and validity of ﬁndings.
Objective: A systematic literature review was conducted to investigate the extent to which LCA has been used to study population mental health in children, and whether replicable, reliable and valid ﬁndings have been demonstrated.
Methods: Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines were followed. A search of literature, published between January 1998 and December 2017, was carried out using MEDLINE, EMBASE, PsycInfo, Scopus, ERIC, ASSIA, and Google Scholar. A total of 2,748 studies were initially identiﬁed, of which 23 were eligible for review. The review examined the methods which studies had used to choose the number of mental health classes, the classes that they found, and whether there was evidence for the validity and reliability of the classes.
Results: Reviewed studies used LCA to investigate both disparate mental health symptoms, and those associated with speciﬁc disorders. The corpus of studies using similar indicators was small. Differences in the criteria used to select the ﬁnal LCA model were found between studies. All studies found meaningful or useful subgroups, but there were differences in the extent to which the validity and reliability of classes were explicitly demonstrated.
Conclusions : LCA is a useful tool for studying and classifying child mental health at the population level. Recommendations are made to improve the application and reporting of LCA and to increase conﬁdence in ﬁndings in the future, including use of a range of indices and criteria when enumerating classes, clear reporting of methods for replicability, and making efforts to establish the validity and reliability of identiﬁed classes.},
	language = {en},
	urldate = {2021-09-21},
	journal = {Frontiers in Psychology},
	author = {Petersen, Kimberly J. and Qualter, Pamela and Humphrey, Neil},
	month = may,
	year = {2019},
	pages = {1214},
	file = {Petersen et al_2019_The Application of Latent Class Analysis for Investigating Population Child.pdf:/Users/tasospsy/Google Drive/New_Zotero/Petersen et al_2019_The Application of Latent Class Analysis for Investigating Population Child.pdf:application/pdf},
}

@article{delatorre_generalized_2011,
	title = {The {Generalized} {DINA} {Model} {Framework}},
	volume = {76},
	issn = {1860-0980},
	url = {https://doi.org/10.1007/s11336-011-9207-7},
	doi = {10.1007/s11336-011-9207-7},
	abstract = {The G-DINA (generalized deterministic inputs, noisy “and” gate) model is a generalization of the DINA model with more relaxed assumptions. In its saturated form, the G-DINA model is equivalent to other general models for cognitive diagnosis based on alternative link functions. When appropriate constraints are applied, several commonly used cognitive diagnosis models (CDMs) can be shown to be special cases of the general models. In addition to model formulation, the G-DINA model as a general CDM framework includes a component for item-by-item model estimation based on design and weight matrices, and a component for item-by-item model comparison based on the Wald test. The paper illustrates the estimation and application of the G-DINA model as a framework using real and simulated data. It concludes by discussing several potential implications of and relevant issues concerning the proposed framework.},
	language = {en},
	number = {2},
	urldate = {2021-11-02},
	journal = {Psychometrika},
	author = {de la Torre, Jimmy},
	month = apr,
	year = {2011},
	pages = {179--199},
	file = {de la Torre_2011_The Generalized DINA Model Framework.pdf:/Users/tasospsy/Google Drive/New_Zotero/de la Torre_2011_The Generalized DINA Model Framework.pdf:application/pdf},
}

@book{kaplan_sage_2004,
	address = {2455 Teller Road, Thousand Oaks California 91320 United States of America},
	title = {The {SAGE} {Handbook} of {Quantitative} {Methodology} for the {Social} {Sciences}},
	isbn = {978-0-7619-2359-6 978-1-4129-8631-1},
	url = {http://methods.sagepub.com/book/the-sage-handbook-of-quantitative-methodology-for-the-social-sciences},
	language = {en},
	urldate = {2021-10-09},
	publisher = {SAGE Publications, Inc.},
	author = {Kaplan, David},
	year = {2004},
	doi = {10.4135/9781412986311},
	file = {Kaplan_2004_The SAGE Handbook of Quantitative Methodology for the Social Sciences.pdf:/Users/tasospsy/Google Drive/New_Zotero/Kaplan_2004_The SAGE Handbook of Quantitative Methodology for the Social Sciences.pdf:application/pdf},
}

@article{vermunt_use_2001,
	title = {The {Use} of {Restricted} {Latent} {Class} {Models} for {Defining} and {Testing} {Nonparametric} and {Parametric} {Item} {Response} {Theory} {Models}},
	volume = {25},
	issn = {0146-6216},
	url = {https://doi.org/10.1177/01466210122032082},
	doi = {10.1177/01466210122032082},
	abstract = {A general class of ordinal logit models is presented that specifies equality and inequality constraints on sums of conditional response probabilities. Using these constraints in latent class analysis, models are obtained that are similar to parametric and nonparametric item response models. Maximum likelihood is used to estimate these models, making their assumptions testable with likelihood-ratio statistics. Because of the intractability of the asymptotic distribution of the goodness-of-fit measure when imposing inequality constraints, parametric bootstrapping is used to obtain estimates of p values. The proposed restricted latent class models are illustrated by an example using reported adult crying behavior.},
	number = {3},
	urldate = {2021-11-02},
	journal = {Applied Psychological Measurement},
	author = {Vermunt, Jeroen K.},
	month = sep,
	year = {2001},
	note = {Publisher: SAGE Publications Inc},
	pages = {283--294},
	file = {Vermunt_2001_The Use of Restricted Latent Class Models for Defining and Testing.pdf:/Users/tasospsy/Google Drive/New_Zotero/Vermunt_2001_The Use of Restricted Latent Class Models for Defining and Testing.pdf:application/pdf},
}

@article{cheng_when_2009,
	title = {When {Cognitive} {Diagnosis} {Meets} {Computerized} {Adaptive} {Testing}: {CD}-{CAT}},
	volume = {74},
	issn = {0033-3123, 1860-0980},
	shorttitle = {When {Cognitive} {Diagnosis} {Meets} {Computerized} {Adaptive} {Testing}},
	url = {http://link.springer.com/10.1007/s11336-009-9123-2},
	doi = {10.1007/s11336-009-9123-2},
	abstract = {Computerized adaptive testing (CAT) is a mode of testing which enables more efﬁcient and accurate recovery of one or more latent traits. Traditionally, CAT is built upon Item Response Theory (IRT) models that assume unidimensionality. However, the problem of how to build CAT upon latent class models (LCM) has not been investigated until recently, when Tatsuoka (J. R. Stat. Soc., Ser. C, Appl. Stat. 51:337–350, 2002) and Tatsuoka and Ferguson (J. R. Stat., Ser. B 65:143–157, 2003) established a general theorem on the asymptotically optimal sequential selection of experiments to classify ﬁnite, partially ordered sets. Xu, Chang, and Douglas (Paper presented at the annual meeting of National Council on Measurement in Education, Montreal, Canada, 2003) then tested two heuristics in a simulation study based on Tatsuoka’s theoretical work in the context of computerized adaptive testing. One of the heuristics was developed based on Kullback–Leibler information, and the other based on Shannon entropy. In this paper, we showcase the application of the optimal sequential selection methodology in item selection of CAT that is built upon cognitive diagnostic models. Two new heuristics are proposed, and are compared against the randomized item selection method and the two heuristics investigated in Xu et al. (Paper presented at the annual meeting of National Council on Measurement in Education, Montreal, Canada, 2003). Finally, we show the connection between the Kullback–Leibler-information-based approaches and the Shannonentropy-based approach, as well as the connection between algorithms built upon LCM and those built upon IRT models.},
	language = {en},
	number = {4},
	urldate = {2021-11-02},
	journal = {Psychometrika},
	author = {Cheng, Ying},
	month = dec,
	year = {2009},
	pages = {619--632},
	file = {Cheng_2009_When Cognitive Diagnosis Meets Computerized Adaptive Testing.pdf:/Users/tasospsy/Google Drive/New_Zotero/Cheng_2009_When Cognitive Diagnosis Meets Computerized Adaptive Testing.pdf:application/pdf},
}

@article{bollen_comparison_2012,
	title = {A {Comparison} of {Bayes} {Factor} {Approximation} {Methods} {Including} {Two} {New} {Methods}},
	volume = {41},
	issn = {0049-1241},
	url = {https://doi.org/10.1177/0049124112452393},
	doi = {10.1177/0049124112452393},
	abstract = {Bayes factors (BFs) play an important role in comparing the fit of statistical models. However, computational limitations or lack of an appropriate prior sometimes prevent researchers from using exact BFs. Instead, it is approximated, often using the Bayesian Information Criterion (BIC) or a variant of BIC. The authors provide a comparison of several BF approximations, including two new approximations, the Scaled Unit Information Prior Bayesian Information Criterion (SPBIC) and Information matrix-based Bayesian Information Criterion (IBIC). The SPBIC uses a scaled unit information prior that is more general than the BIC?s unit information prior, and the IBIC utilizes more terms of approximation than the BIC. Through simulation, the authors show that several measures perform well in large samples, that performance declines in smaller samples, and that SPBIC and IBIC provide improvement to existing measures under some conditions, including small sample sizes. The authors illustrate the use of the fit measures with the crime data of Ehrlich and then conclude with recommendations for researchers.},
	number = {2},
	urldate = {2021-12-07},
	journal = {Sociological Methods \& Research},
	author = {Bollen, Kenneth A. and Ray, Surajit and Zavisca, Jane and Harden, Jeffrey J.},
	month = may,
	year = {2012},
	note = {Publisher: SAGE Publications Inc},
	pages = {294--324},
	file = {SAGE PDF Full Text:/Users/tasospsy/Zotero/storage/MWH2H6BZ/Bollen et al. - 2012 - A Comparison of Bayes Factor Approximation Methods.pdf:application/pdf},
}

@article{marcoulides_residual-based_2021,
	title = {Residual-{Based} {Algorithm} for {Growth} {Mixture} {Modeling}: {A} {Monte} {Carlo} {Simulation} {Study}},
	volume = {12},
	issn = {1664-1078},
	shorttitle = {Residual-{Based} {Algorithm} for {Growth} {Mixture} {Modeling}},
	url = {https://www.frontiersin.org/article/10.3389/fpsyg.2021.618647},
	doi = {10.3389/fpsyg.2021.618647},
	abstract = {Growth mixture models are regularly applied in the behavioral and social sciences to identify unknown heterogeneous subpopulations that follow distinct developmental trajectories. Marcoulides and Trinchera (2019) recently proposed a mixture modeling approach that examines the presence of multiple latent classes by algorithmically grouping or clustering individuals who follow the same estimated growth trajectory based on an evaluation of individual case residuals. The purpose of this article was to conduct a simulation study that examines the performance of this new approach for determining the number of classes in growth mixture models. The performance of the approach to correctly identify the number of classes is examined under a variety of longitudinal data design conditions. The findings demonstrated that the new approach was a very dependable indicator of classes across all the design conditions considered.},
	urldate = {2021-12-08},
	journal = {Frontiers in Psychology},
	author = {Marcoulides, Katerina M. and Trinchera, Laura},
	year = {2021},
	pages = {550},
	file = {Marcoulides_Trinchera_2021_Residual-Based Algorithm for Growth Mixture Modeling.pdf:/Users/tasospsy/Google Drive/New_Zotero/Marcoulides_Trinchera_2021_Residual-Based Algorithm for Growth Mixture Modeling.pdf:application/pdf},
}

@article{tein_statistical_2013,
	title = {Statistical {Power} to {Detect} the {Correct} {Number} of {Classes} in {Latent} {Profile} {Analysis}},
	volume = {20},
	issn = {1070-5511},
	url = {https://doi.org/10.1080/10705511.2013.824781},
	doi = {10.1080/10705511.2013.824781},
	abstract = {Little research has examined factors influencing statistical power to detect the correct number of latent classes using latent profile analysis (LPA). This simulation study examined power related to interclass distance between latent classes given true number of classes, sample size, and number of indicators. Seven model selection methods were evaluated. None had adequate power to select the correct number of classes with a small (Cohen's d = .2) or medium (d = .5) degree of separation. With a very large degree of separation (d = 1.5), the Lo–Mendell–Rubin test (LMR), adjusted LMR, bootstrap likelihood ratio test, Bayesian Information Criterion (BIC), and sample-size-adjusted BIC were good at selecting the correct number of classes. However, with a large degree of separation (d = .8), power depended on number of indicators and sample size. Akaike's Information Criterion and entropy poorly selected the correct number of classes, regardless of degree of separation, number of indicators, or sample size.},
	number = {4},
	urldate = {2021-12-08},
	journal = {Structural Equation Modeling: A Multidisciplinary Journal},
	author = {Tein, Jenn-Yun and Coxe, Stefany and Cham, Heining},
	month = oct,
	year = {2013},
	pmid = {24489457},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/10705511.2013.824781},
	keywords = {interclass distance, latent profile analysis, mixture models, model section methods, power},
	pages = {640--657},
	file = {Tein et al_2013_Statistical Power to Detect the Correct Number of Classes in Latent Profile.pdf:/Users/tasospsy/Google Drive/New_Zotero/Tein et al_2013_Statistical Power to Detect the Correct Number of Classes in Latent Profile.pdf:application/pdf;Snapshot:/Users/tasospsy/Zotero/storage/EIM6BYKI/10705511.2013.html:text/html},
}

@incollection{van_groen_multidimensional_2019,
	address = {Cham},
	series = {Methodology of {Educational} {Measurement} and {Assessment}},
	title = {Multidimensional {Computerized} {Adaptive} {Testing} for {Classifying} {Examinees}},
	isbn = {978-3-030-18480-3},
	url = {https://doi.org/10.1007/978-3-030-18480-3_14},
	abstract = {Multidimensional computerized classification testing can be used when classification decisions are required for constructs that have a multidimensional structure. Here, two methods for making those decisions are included for two types of multidimensionality. In the case of between-item multidimensionality, each item is intended to measure just one dimension. In the case of within-item multidimensionality, items are intended to measure multiple or all dimensions. Wald’s (1947) sequential probability ratio test and Kingsbury and Weiss (1979) confidence interval method can be applied to multidimensional classification testing. Three methods are included for selecting the items: random item selection, maximization at the current ability estimate, and the weighting method. The last method maximizes information based on a combination of the cutoff points weighted by their distance to the ability estimate. Two examples illustrate the use of the classification and item selection methods.},
	language = {en},
	urldate = {2021-12-08},
	booktitle = {Theoretical and {Practical} {Advances} in {Computer}-based {Educational} {Measurement}},
	publisher = {Springer International Publishing},
	author = {van Groen, Maaike M. and Eggen, Theo J. H. M. and Veldkamp, Bernard P.},
	editor = {Veldkamp, Bernard P. and Sluijter, Cor},
	year = {2019},
	doi = {10.1007/978-3-030-18480-3_14},
	pages = {271--289},
	file = {van Groen et al_2019_Multidimensional Computerized Adaptive Testing for Classifying Examinees.pdf:/Users/tasospsy/Google Drive/New_Zotero/van Groen et al_2019_Multidimensional Computerized Adaptive Testing for Classifying Examinees.pdf:application/pdf},
}

@article{schermelleh-engel_evaluating_2003,
	title = {Evaluating the {Fit} of {Structural} {Equation} {Models}: {Tests} of {Significance} and {Descriptive} {Goodness}-of-{Fit} {Measures}},
	volume = {8},
	abstract = {For structural equation models, a huge variety of fit indices has been developed. These indices, however, can point to conflicting conclusions about the extent to which a model actually matches the observed data. The present article provides some guidelines that should help applied researchers to evaluate the adequacy of a given structural equation model. First, as goodness-of-fit measures depend on the method used for parameter estimation, maximum likelihood (ML) and weighted least squares (WLS) methods are introduced in the context of structural equation modeling. Then, the most common goodness-of-fit indices are discussed and some recommendations for practitioners given. Finally, we generated an artificial data set according to a "true" model and analyzed two misspecified and two correctly specified models as examples of poor model fit, adequate fit, and good fit.},
	language = {en},
	number = {2},
	author = {Schermelleh-Engel, Karin and Moosbrugger, Helfried and Müller, Hans},
	year = {2003},
	pages = {52},
	file = {Schermelleh-Engel et al_2003_Evaluating the Fit of Structural Equation Models.pdf:/Users/tasospsy/Google Drive/New_Zotero/Schermelleh-Engel et al_2003_Evaluating the Fit of Structural Equation Models.pdf:application/pdf},
}

@article{morgan_mixed_2015,
	title = {Mixed {Mode} {Latent} {Class} {Analysis}: {An} {Examination} of {Fit} {Index} {Performance} for {Classification}},
	volume = {22},
	issn = {1070-5511, 1532-8007},
	shorttitle = {Mixed {Mode} {Latent} {Class} {Analysis}},
	url = {http://www.tandfonline.com/doi/abs/10.1080/10705511.2014.935751},
	doi = {10.1080/10705511.2014.935751},
	language = {en},
	number = {1},
	urldate = {2021-12-08},
	journal = {Structural Equation Modeling: A Multidisciplinary Journal},
	author = {Morgan, Grant B.},
	month = jan,
	year = {2015},
	pages = {76--86},
	file = {10705511.2014.pdf:/Users/tasospsy/Zotero/storage/2443SVPM/10705511.2014.pdf:application/pdf},
}

@article{vrieze_model_2012,
	title = {Model selection and psychological theory: {A} discussion of the differences between the {Akaike} {Information} {Criterion} ({AIC}) and the {Bayesian} {Information} {Criterion} ({BIC})},
	volume = {17},
	issn = {1082-989X},
	shorttitle = {Model selection and psychological theory},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3366160/},
	doi = {10.1037/a0027127},
	abstract = {This article reviews the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC) in model selection and the appraisal of psychological theory. The focus is on latent variable models given their growing use in theory testing and construction. We discuss theoretical statistical results in regression and illustrate more important issues with novel simulations involving latent variable models including factor analysis, latent profile analysis, and factor mixture models. Asymptotically, the BIC is consistent, in that it will select the true model if, among other assumptions, the true model is among the candidate models considered. The AIC is not consistent under these circumstances. When the true model is not in the candidate model set the AIC is effcient, in that it will asymptotically choose whichever model minimizes the mean squared error of prediction/estimation. The BIC is not effcient under these circumstances. Unlike the BIC, the AIC also has a minimax property, in that it can minimize the maximum possible risk in finite sample sizes. In sum, the AIC and BIC have quite different properties that require different assumptions, and applied researchers and methodologists alike will benefit from improved understanding of the asymptotic and finite-sample behavior of these criteria. The ultimate decision to use AIC or BIC depends on many factors, including: the loss function employed, the study's methodological design, the substantive research question, and the notion of a true model and its applicability to the study at hand.},
	number = {2},
	urldate = {2021-12-08},
	journal = {Psychological Methods},
	author = {Vrieze, Scott I.},
	month = jun,
	year = {2012},
	pmid = {22309957},
	pmcid = {PMC3366160},
	pages = {228--243},
	file = {Vrieze_2012_Model selection and psychological theory.pdf:/Users/tasospsy/Google Drive/New_Zotero/Vrieze_2012_Model selection and psychological theory.pdf:application/pdf},
}

@article{bozdogan_model_1987,
	title = {Model selection and {Akaike}'s {Information} {Criterion} ({AIC}): {The} general theory and its analytical extensions},
	volume = {52},
	issn = {1860-0980},
	shorttitle = {Model selection and {Akaike}'s {Information} {Criterion} ({AIC})},
	url = {https://doi.org/10.1007/BF02294361},
	doi = {10.1007/BF02294361},
	abstract = {During the last fifteen years, Akaike's entropy-based Information Criterion (AIC) has had a fundamental impact in statistical model evaluation problems. This paper studies the general theory of the AIC procedure and provides its analytical extensions in two ways without violating Akaike's main principles. These extensions make AIC asymptotically consistent and penalize overparameterization more stringently to pick only the simplest of the “true” models. These selection criteria are called CAIC and CAICF. Asymptotic properties of AIC and its extensions are investigated, and empirical performances of these criteria are studied in choosing the correct degree of a polynomial model in two different Monte Carlo experiments under different conditions.},
	language = {en},
	number = {3},
	urldate = {2021-12-08},
	journal = {Psychometrika},
	author = {Bozdogan, Hamparsum},
	month = sep,
	year = {1987},
	pages = {345--370},
	file = {Bozdogan_1987_Model selection and Akaike's Information Criterion (AIC).pdf:/Users/tasospsy/Google Drive/New_Zotero/Bozdogan_1987_Model selection and Akaike's Information Criterion (AIC).pdf:application/pdf},
}

@incollection{akaike_new_1998,
	address = {New York, NY},
	series = {Springer {Series} in {Statistics}},
	title = {A {New} {Look} at the {Statistical} {Model} {Identification}},
	isbn = {978-1-4612-1694-0},
	url = {https://doi.org/10.1007/978-1-4612-1694-0_16},
	abstract = {The history of the development of statistical hypothesis testing in time series analysis is reviewed briefly and it is pointed out that the hypothesis testing procedure is not adequately defined as the procedure for statistical model identification. The classical maximum likelihood estimation procedure is reviewed and a new estimate minimum information theoretical criterion (AIC) estimate (MAICE) which is designed for the purpose of statistical identification is introduced. When there are several competing models the MAICE is defined by the model and the maximum likelihood estimates of the parameters which give the minimum of AIC defined by AIC = (−2)log- (maximum likelihood) + 2(number of independently adjusted parameters within the model). MAICE provides a versatile procedure for statistical model identification which is free from the ambiguities inherent in the application of conventional hypothesis testing procedure. The practical utility of MAICE in time series analysis is demonstrated with some numerical examples.},
	language = {en},
	urldate = {2021-12-08},
	booktitle = {Selected {Papers} of {Hirotugu} {Akaike}},
	publisher = {Springer},
	author = {Akaike, Hirotugu},
	editor = {Parzen, Emanuel and Tanabe, Kunio and Kitagawa, Genshiro},
	year = {1998},
	doi = {10.1007/978-1-4612-1694-0_16},
	keywords = {Classical Maximum Likelihood, Final Prediction Error, Gaussian Process Model, Hankel Matrix, Time Series Analysis},
	pages = {215--222},
}

@article{schwarz_estimating_1978,
	title = {Estimating the {Dimension} of a {Model}},
	volume = {6},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/journals/annals-of-statistics/volume-6/issue-2/Estimating-the-Dimension-of-a-Model/10.1214/aos/1176344136.full},
	doi = {10.1214/aos/1176344136},
	abstract = {The problem of selecting one of a number of models of different dimensions is treated by finding its Bayes solution, and evaluating the leading terms of its asymptotic expansion. These terms are a valid large-sample criterion beyond the Bayesian context, since they do not depend on the a priori distribution.},
	number = {2},
	urldate = {2021-12-08},
	journal = {The Annals of Statistics},
	author = {Schwarz, Gideon},
	month = mar,
	year = {1978},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {62F99, 62J99, Akaike information criterion, asymptotics, dimension},
	pages = {461--464},
	file = {Schwarz_1978_Estimating the Dimension of a Model.pdf:/Users/tasospsy/Google Drive/New_Zotero/Schwarz_1978_Estimating the Dimension of a Model.pdf:application/pdf;Snapshot:/Users/tasospsy/Zotero/storage/D9CS8LXM/1176344136.html:text/html},
}

@article{sclove_application_1987,
	title = {Application of model-selection criteria to some problems in multivariate analysis},
	volume = {52},
	issn = {1860-0980},
	doi = {10.1007/BF02294360},
	abstract = {A review of model-selection criteria is presented, with a view toward showing their similarities. It is suggested that some problems treated by sequences of hypothesis tests may be more expeditiously treated by the application of model-selection criteria. Consideration is given to application of model-selection criteria to some problems of multivariate analysis, especially the clustering of variables, factor analysis, and general description of a complex of variables. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	number = {3},
	journal = {Psychometrika},
	author = {Sclove, Stanley L.},
	year = {1987},
	note = {Place: Germany
Publisher: Springer},
	keywords = {Multivariate Analysis, Factor Analysis, Cluster Analysis, Mathematical Modeling},
	pages = {333--343},
	file = {Snapshot:/Users/tasospsy/Zotero/storage/7Z3E99SD/1989-17697-001.html:text/html},
}

@article{noauthor_computerized_2021,
	title = {Computerized adaptive testing without {IRT} for flexible measurement and prediction},
	url = {https://osf.io/89wr2/},
	abstract = {Hosted on the Open Science Framework},
	language = {en},
	urldate = {2021-12-09},
	month = aug,
	year = {2021},
	note = {Publisher: OSF},
	file = {Snapshot:/Users/tasospsy/Zotero/storage/LLK2594T/q3y5a.html:text/html},
}

@book{magis_computerized_2017,
	address = {Cham},
	series = {Use {R}!},
	title = {Computerized {Adaptive} and {Multistage} {Testing} with {R}},
	isbn = {978-3-319-69217-3 978-3-319-69218-0},
	url = {http://link.springer.com/10.1007/978-3-319-69218-0},
	language = {en},
	urldate = {2021-12-09},
	publisher = {Springer International Publishing},
	author = {Magis, David and Yan, Duanli and von Davier, Alina A.},
	year = {2017},
	doi = {10.1007/978-3-319-69218-0},
	file = {CAT and Multistage in R.pdf:/Users/tasospsy/Zotero/storage/7MNLLQWA/CAT and Multistage in R.pdf:application/pdf},
}

@book{wainer_computerized_2000,
	address = {Mahwah, N.J},
	edition = {2nd ed},
	title = {Computerized adaptive testing: a primer},
	isbn = {978-0-8058-3511-3},
	shorttitle = {Computerized adaptive testing},
	language = {en},
	publisher = {Lawrence Erlbaum Associates},
	author = {Wainer, Howard and Dorans, Neil J.},
	year = {2000},
	keywords = {Computer adaptive testing},
	file = {Computerized adaptive testing_ a primer ( PDFDrive ).pdf:/Users/tasospsy/Zotero/storage/RTCM959P/Computerized adaptive testing_ a primer ( PDFDrive ).pdf:application/pdf},
}

@book{van_der_linden_elements_2010,
	address = {New York, NY},
	title = {Elements of {Adaptive} {Testing}},
	isbn = {978-0-387-85459-5 978-0-387-85461-8},
	url = {http://link.springer.com/10.1007/978-0-387-85461-8},
	language = {en},
	urldate = {2021-12-09},
	publisher = {Springer New York},
	editor = {van der Linden, Wim J. and Glas, Cees A.W.},
	year = {2010},
	doi = {10.1007/978-0-387-85461-8},
	file = {Elements of adaptive testing.pdf:/Users/tasospsy/Zotero/storage/IHV9RF7R/Elements of adaptive testing.pdf:application/pdf},
}

@article{zijlstra_outlier_2007,
	title = {Outlier {Detection} in {Test} and {Questionnaire} {Data}},
	volume = {42},
	issn = {0027-3171, 1532-7906},
	url = {https://www.tandfonline.com/doi/full/10.1080/00273170701384340},
	doi = {10.1080/00273170701384340},
	language = {en},
	number = {3},
	urldate = {2021-12-10},
	journal = {Multivariate Behavioral Research},
	author = {Zijlstra, Wobbe P. and van der Ark, L. Andries and Sijtsma, Klaas},
	month = oct,
	year = {2007},
	pages = {531--555},
	file = {00273170701384340.pdf:/Users/tasospsy/Zotero/storage/GXC4EW83/00273170701384340.pdf:application/pdf},
}

@misc{noauthor_apa_nodate,
	title = {{APA} {PsycNet}},
	url = {https://psycnet.apa.org/home},
	abstract = {APA PsycNet},
	language = {en},
	urldate = {2021-12-15},
	file = {Snapshot:/Users/tasospsy/Zotero/storage/PHQSPQDQ/home.html:text/html},
}

@book{furr_psychometrics_2013,
	title = {Psychometrics: {An} {Introduction}},
	isbn = {978-1-4833-2192-9},
	url = {https://books.google.nl/books?id=BbwgAQAAQBAJ},
	publisher = {SAGE Publications},
	author = {Furr, R.M. and Bacharach, V.R.},
	year = {2013},
}

@article{jeon_latent_2020,
	title = {Latent {Class} {Analysis} for {Repeatedly} {Measured} {Multiple} {Latent} {Class} {Variables}},
	volume = {0},
	issn = {0027-3171},
	url = {https://doi.org/10.1080/00273171.2020.1848515},
	doi = {10.1080/00273171.2020.1848515},
	abstract = {Research on stage-sequential shifts across multiple latent classes can be challenging in part because it may not be possible to observe the particular stage-sequential pattern of a single latent class variable directly. In addition, one latent class variable may affect or be affected by other latent class variables and the associations among multiple latent class variables are not likely to be directly observed either. To address this difficulty, we propose a multivariate latent class analysis for longitudinal data, joint latent class profile analysis (JLCPA), which provides a principle for the systematic identification of not only associations among multiple discrete latent variables but sequential patterns of those associations. We also propose the recursive formula to the EM algorithm to overcome the computational burden in estimating the model parameters, and our simulation study shows that the proposed algorithm is much faster in computing estimates than the standard EM method. In this work, we apply a JLCPA using data from the National Longitudinal Survey of Youth 1997 in order to investigate the multiple drug-taking behavior of early-onset drinkers from their adolescence, via young adulthood, to adulthood.},
	number = {0},
	urldate = {2022-01-26},
	journal = {Multivariate Behavioral Research},
	author = {Jeon, Saebom and Seo, Tae Seok and Anthony, James C. and Chung, Hwan},
	month = aug,
	year = {2020},
	pmid = {33236935},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/00273171.2020.1848515},
	keywords = {Drug-taking behavior, longitudinal data, multivariate latent classes, recursive EM, stage-sequential process},
	pages = {1--15},
	file = {Jeon et al_2020_Latent Class Analysis for Repeatedly Measured Multiple Latent Class Variables.pdf:/Users/tasospsy/Google Drive/New_Zotero/Jeon et al_2020_Latent Class Analysis for Repeatedly Measured Multiple Latent Class Variables.pdf:application/pdf;Snapshot:/Users/tasospsy/Zotero/storage/WLYXT8FQ/00273171.2020.html:text/html},
}
